{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import os\n",
    "from glob import glob\n",
    "import yaml\n",
    "\n",
    "def plot_spec(specs, titles):\n",
    "    fig, axs = plt.subplots(1, len(specs))\n",
    "    for i, spec in enumerate(specs):\n",
    "        axs[i].imshow(spec.detach().cpu().numpy(),\n",
    "                      origin=\"lower\", aspect=\"auto\", cmap='magma')\n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_title(titles[i])\n",
    "    fig.tight_layout(pad=0)\n",
    "    return fig\n",
    "\n",
    "class SPEC():\n",
    "    def __init__(self, n_fft, overlap=0.75, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.eps = eps\n",
    "        self.hop_length = int(n_fft * (1 - overlap))  # 25% of the length\n",
    "        self.spec = torchaudio.transforms.Spectrogram(\n",
    "            n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "\n",
    "    def log_spec(self, x):\n",
    "        S = self.spec(x)\n",
    "        log_S = (S + self.eps).log2()\n",
    "        return log_S\n",
    "\n",
    "\n",
    "def load_audio(audio_dir):\n",
    "    subdirs = glob(audio_dir + \"/*\")\n",
    "    audios = []\n",
    "    forces = []\n",
    "    for sspath in subdirs:\n",
    "        files = os.listdir(sspath)\n",
    "        gain = [1, 1]\n",
    "        pad = [0, 0]\n",
    "        for filename in files:\n",
    "            filedir = sspath + \"/\" + filename\n",
    "            if \"mic\" in filename:\n",
    "                audio, sr = torchaudio.load(filedir)\n",
    "            if \"Force\" in filename:\n",
    "                force, sr = torchaudio.load(filedir)\n",
    "            if \"metadata\" in filename:\n",
    "                f = open(filedir)\n",
    "                yaml_data = yaml.safe_load(f)\n",
    "                gain = yaml_data.get(\"gain\")\n",
    "                pad = yaml_data.get(\"pad\")\n",
    "\n",
    "        force = torchaudio.functional.gain(force, gain[0])\n",
    "        audio = torchaudio.functional.gain(audio, gain[1])\n",
    "        force = force[:, pad[0] * sr:]\n",
    "        audio = audio[:, pad[1] * sr:]\n",
    "        audios.append(audio[0])  # only use the first channel\n",
    "        forces.append(force[0])  # only use the first channel\n",
    "    return audios, forces, sr\n",
    "\n",
    "spec = SPEC(512)\n",
    "\n",
    "audio_specs = []\n",
    "titles = []\n",
    "for i in range(100):\n",
    "    audio_dir = f'../data/audio_data/{i+1}/audio'\n",
    "    audios, forces, sr = load_audio(audio_dir)\n",
    "    audio_spec = spec.log_spec(audios[0][:sr // 2])\n",
    "    audio_specs.append(audio_spec)\n",
    "    titles.append(f'audio {i+1}')\n",
    "\n",
    "for i in range(20):\n",
    "    fig = plot_spec(audio_specs[i*5:i*5+5], titles[i*5:i*5+5])\n",
    "    fig.savefig(f'specs/audio_spec_{i}.png', dpi=300)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffFEM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
